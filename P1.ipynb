{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.listdir(\"test_images/\")\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING TOOL \n",
    "def apply_brightness_contrast(input_img, brightness = 0, contrast = 0):\n",
    "    \n",
    "    \"\"\"a function to add contrast and brightness: this is only used to as a visual help to fine tune\n",
    "        some of the pipeline parameters.\n",
    "    \"\"\"  \n",
    "\n",
    "    if brightness != 0:\n",
    "        if brightness > 0:\n",
    "            shadow = brightness\n",
    "            highlight = 255\n",
    "        else:\n",
    "            shadow = 0\n",
    "            highlight = 255 + brightness\n",
    "        alpha_b = (highlight - shadow)/255\n",
    "        gamma_b = shadow\n",
    "\n",
    "        buf = cv2.addWeighted(input_img, alpha_b, input_img, 0, gamma_b)\n",
    "    else:\n",
    "        buf = input_img.copy()\n",
    "        #buf=np.copy(input_img)\n",
    "    if contrast != 0:\n",
    "        f = 131*(contrast + 127)/(127*(131-contrast))\n",
    "        alpha_c = f\n",
    "        gamma_c = 127*(1-f)\n",
    "\n",
    "        buf = cv2.addWeighted(buf, alpha_c, buf, 0, gamma_c)\n",
    "\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# COLOR FILTERING FUNCTIONS\n",
    "\n",
    "def HSL_color_picker(H,S,L):\n",
    "    \"\"\"\n",
    "    # HLS values have to beconverted in [0..255] values for cv2 usage \n",
    "    # H value can be arbitrary, [0 ... 360](degrees on the color wheel ) (OpenCV: [0 ... 180]!!!)\n",
    "    # L  [1 ... 100] % on the luminosity axis (OpenCV: [0 ... 255]): low~shade(i.e black),high~color\n",
    "    # S [1 ... 100] % on the satuation axis (OpenCV: [0 ... 255])] low~faded(i.e white) high~normal lighting\"\"\"\n",
    "    \n",
    "    if H not in range(0,361):\n",
    "        print(\"Hue should be in range(0,360) \")\n",
    "\n",
    "    if S not in range(0,255):\n",
    "        print(\"Saturation tu should be in range(0,256) \")\n",
    "    if L not in range(0,255):\n",
    "        print(\"Luminosity should be in range(0,256) \")\n",
    "    return(np.array([np.round( H/2), np.round(255*L/100 ), np.round(255*S/100)]))\n",
    "\n",
    "    \n",
    "    #return(np.array([np.round( H/2), np.round(255*S/100 ), np.round(255*L/100)]))\n",
    "    \n",
    "    #white_upper = np.array([np.round(360 / 2), np.round(1.00 * 255), np.round(0.30 * 255)])\n",
    "\n",
    "\n",
    "def filter_color_mask(image_RGB,lower_filter = np.array([0,0,0]), higher_filter = np.array([0,0,0])):#create a\n",
    "    # \"image\" : format is RGB, \n",
    "    # filters are Hue, Luminosity, Saturation\n",
    "    image_HLS=cv2.cvtColor(image_RGB, cv2.COLOR_RGB2HLS) #conversion to HLS\n",
    "    mask = cv2.inRange(image_HLS, lower_filter, higher_filter)\n",
    "    return(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AveragedBuffer():\n",
    "    \n",
    "    \"\"\"stores values in a buffer(FIFO),limited length, \n",
    "        and return the mean for the non zero values \n",
    "    \"\"\"   \n",
    "    \n",
    "    def __init__(self,length):\n",
    "        \n",
    "        self.length=length\n",
    "        self.deck=deque([],maxlen=self.length)\n",
    "        \n",
    "    def append(self,element):\n",
    "        self.deck.append(element)\n",
    "    \n",
    "    def content(self):\n",
    "        #for testing and tuning purposes\n",
    "        return(list(self.deck))\n",
    "    \n",
    "    def MEAN(self):\n",
    "        v = np.array(self.deck, dtype=np.float32)\n",
    "    \n",
    "        if sum(v)==0:\n",
    "            return(0)\n",
    "        else:\n",
    "            v[v == 0] = np.nan #to exclude the O values from the average calculation \n",
    "            return (np.nanmean(v))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            \n",
    "def line_to_XY(line):\n",
    "    \n",
    "    \"\"\"draws a straight line between a couple of points\"\"\"\n",
    "    \n",
    "    (x1,y1,x2,y2)=line[0]                   \n",
    "    X=[x1,x2]\n",
    "    Y=[y1,y2]\n",
    "    return(X,Y)\n",
    "    \n",
    "def slope_line(line):\n",
    "    \n",
    "    \"\"\"calculates the slope and the intercept passing through a couple of points\"\"\"\n",
    "    \n",
    "    (X,Y)=line_to_XY(line)\n",
    "    return(polyfit(X,Y,1))    \n",
    "\n",
    "\n",
    "   \n",
    "def STD_filtering(img,lines,DEVIATION_FACTOR=4,color=[0,255,0],thickness=5):\n",
    "    \n",
    "    \"\"\"filters out oulliers from a list of point couples\n",
    "    returns the slope and intercept of the best fit line for the remaining pony couples\"\"\"\n",
    "    \n",
    "    if lines is None:\n",
    "        return(0,0)\n",
    "    else:\n",
    "        N_lines=lines\n",
    "        # calculate the slope for each segments on this side:\n",
    "        N_SLOPES=list(map(lambda line:(slope_line(line))[0],N_lines))\n",
    "        \n",
    "        # calculates the mean for all slopes encountered on this side:\n",
    "        MEAN_SLOPE=np.mean(N_SLOPES)\n",
    "    \n",
    "        # calculates the corresponding standard deviation: \n",
    "        STD_DEV_m=np.std(N_SLOPES) \n",
    "        \n",
    "        # finds out if each segment is within a span centered around the mean slope(FYI: \"<=\" make it work when only one segment(STD=0)\n",
    "        IN_STD_DEV_m=lambda line:MEAN_SLOPE-(DEVIATION_FACTOR*1*(STD_DEV_m))<=(slope_line(line))[0]<=MEAN_SLOPE+(DEVIATION_FACTOR*1*(STD_DEV_m))\n",
    "    \n",
    "        # filters out each segment whose slope is not within the mean centered span:\n",
    "        LINES_IN_STD_DEV_m=list(filter(IN_STD_DEV_m,N_lines))#excluding the lines not within a DEVIATION_FACTOR span centered around MEAN_SLOPE\n",
    "        N_SLOPES_in_std=list(map(lambda line:(slope_line(line))[0],LINES_IN_STD_DEV_m))\n",
    "        \n",
    "\n",
    "        # calculates the intercept for each remaining segments on this side (reminder: slope_line(line)[1] is the intercept)\n",
    "        N_INTERCEPTS=list(map(lambda line:slope_line(line)[1],LINES_IN_STD_DEV_m))\n",
    "    \n",
    "        # calculates the mean for all corresponding intercepts remaining on this side:\n",
    "        MEAN_INTERCEPT=np.mean(N_INTERCEPTS)\n",
    "    \n",
    "        # calculates the corresponding intercept standard deviation:\n",
    "        STD_DEV_b=np.std(N_INTERCEPTS) #(mx+b)\n",
    "    \n",
    "        # finds out if each segment is within a span centered around the mean intercept:\n",
    "        IN_STD_DEV_b=lambda line:MEAN_INTERCEPT-(DEVIATION_FACTOR*2*(STD_DEV_b))<=(slope_line(line))[1]<=MEAN_INTERCEPT+(DEVIATION_FACTOR*2*(STD_DEV_b))\n",
    "\n",
    "        # filters out each segment whose intercept is not within the mean centered span:\n",
    "        LINES_IN_STD_DEV_b=list(filter(IN_STD_DEV_b,LINES_IN_STD_DEV_m))\n",
    "    \n",
    "        # we'll use linear regression to find out the best fitting lines passing through the remaining segments \n",
    "    \n",
    "        if len(LINES_IN_STD_DEV_b)>0: # only create lines when sub-lines have been detected on this side\n",
    "            X_N=np.array([])\n",
    "            Y_N=np.array([])\n",
    "    \n",
    "            for line in LINES_IN_STD_DEV_b:\n",
    "                X_N=np.append(X_N,line_to_XY(line)[0])\n",
    "                Y_N=np.append(Y_N,line_to_XY(line)[1])\n",
    "   \n",
    "            (m,b)=polyfit(X_N,Y_N,1) #linear regression\n",
    "    \n",
    "            return(m,b)\n",
    "     \n",
    " \n",
    "        else:\n",
    "            return(0,0) #no relevant average line was detected on this side of this image  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_side_Top2Bottom(img,avg_slope,avg_intercept,m,b,color=[255,0,0]):\n",
    "    \n",
    "    \"\"\"average the last 10 consecutive slopes and intercept\"\"\"  \n",
    "    \n",
    "    if m!=0 and b!=0:\n",
    "        if avg_slope.MEAN()!=0:\n",
    "            if abs((m-avg_slope.MEAN())/avg_slope.MEAN())<0.2:\n",
    "            #only non erratic slope are added to the average\n",
    "                avg_slope.append(m)\n",
    "                avg_intercept.append(b)\n",
    "        else:\n",
    "            avg_slope.append(m)\n",
    "            avg_intercept.append(b)\n",
    "               \n",
    "            \n",
    "        m=avg_slope.MEAN()\n",
    "        b=avg_intercept.MEAN()\n",
    "        \n",
    "    if m!=0 and b!=0:\n",
    "        NL_bottom=(int (polyval([m**-1, -b/m],YBOT)), int(YBOT))\n",
    "        NL_top=(int (polyval([m**-1, -b/m],YTOP)), int(YTOP))         \n",
    "        cv2.line(img,NL_bottom,NL_top,color=[255,0,255],thickness=10) \n",
    "    else :\n",
    "        print(\"no line detected \")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_view(img, lines, thickness=5):\n",
    "    \n",
    "    \"\"\"splits a list of point couples (right or left)\n",
    "    send these lists for further processing: STD filtering, slope and intercept filtering\n",
    "    and lane drawing\"\"\"\n",
    "         \n",
    "    if lines is None:\n",
    "        \n",
    "        # no line was detected on this image\n",
    "        draw_side_Top2Bottom(img,right_average_SLOPE,right_average_INTERCEPT,m=0,b=0)\n",
    "        draw_side_Top2Bottom(img,left_average_SLOPE,left_average_INTERCEPT,m=0,b=0)\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        #right side:\n",
    "        R_lines=list(filter(lambda line:slope_line(line)[0]>0, lines))\n",
    "        \n",
    "        (m,b)=STD_filtering(img,R_lines,DEVIATION_FACTOR=1,color=[255,0,0],thickness=5)\n",
    "        draw_side_Top2Bottom(img,right_average_SLOPE,right_average_INTERCEPT,m,b)\n",
    "      \n",
    "        #left side:\n",
    "        L_lines=list(filter(lambda line:slope_line(line)[0]<0, lines))\n",
    "        (m,b)=STD_filtering(img,L_lines,DEVIATION_FACTOR=2,color=[255,0,0],thickness=5)\n",
    "        draw_side_Top2Bottom(img,left_average_SLOPE,left_average_INTERCEPT,m,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AveragedBuffer():\n",
    "    \n",
    "    \"\"\"stores values in a buffer(FIFO),limited length, \n",
    "        and return the mean for the non zero values \n",
    "    \"\"\"   \n",
    "    \n",
    "    def __init__(self,length):\n",
    "        \n",
    "        self.length=length\n",
    "        self.deck=deque([],maxlen=self.length)\n",
    "        \n",
    "    def append(self,element):\n",
    "        self.deck.append(element)\n",
    "    \n",
    "    def content(self):\n",
    "        #for testing and tuning purposes\n",
    "        return(list(self.deck))\n",
    "    \n",
    "    def MEAN(self):\n",
    "        v = np.array(self.deck, dtype=np.float32)\n",
    "    \n",
    "        if sum(v)==0:\n",
    "            return(0)\n",
    "        else:\n",
    "            v[v == 0] = np.nan #to exclude the O values from the average calculation \n",
    "            return (np.nanmean(v))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    `vertices` should be a numpy array of integer points.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), min_line_len, max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    \n",
    "    split_view(line_img,lines)\n",
    "\n",
    "    return(line_img)\n",
    "\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + γ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, γ)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    " \n",
    "    if image.shape[:1]!=(540, 960):\n",
    "        image=cv2.resize(image,(960,540))\n",
    "        \n",
    "    image_sharper=apply_brightness_contrast(cv2.medianBlur(image,5),0,100)\n",
    "\n",
    "    white_mask=filter_color_mask(image,lower_white,higher_white)\n",
    "    yellow_mask = filter_color_mask(image,lower_yellow,higher_yellow)\n",
    "\n",
    "    mask = white_mask+yellow_mask\n",
    "    mask=mask.astype(np.uint8)\n",
    "    \n",
    "    image_color_mask=cv2.bitwise_and(image_sharper, image_sharper, mask = mask)\n",
    "\n",
    "    #image_color_mask=apply_masks_OR(image_sharper,mask_1=white_mask,mask_2=yellow_mask) \n",
    "\n",
    "    Gray_Ex=grayscale(image_color_mask) # tranform it in a 2D(8bits) gray image\n",
    "\n",
    "\n",
    "    Blurred_Gray=gaussian_blur(Gray_Ex, 5) # convolves averaging square size 5 filter\n",
    "    \n",
    "    Edges=cv2.Canny(Blurred_Gray, 50, 100)\n",
    "    \n",
    "    Masked_Edges=region_of_interest(Edges,VERTICES) #  region_of_interest(mask, vertices)\n",
    "    \n",
    "    cv2.polylines(image, VERTICES,False,thickness=1,color=(255, 255, 255), lineType=cv2.LINE_AA)\n",
    "\n",
    "    \n",
    "    Hough_Lines=hough_lines(Masked_Edges, 1, np.pi/180, 12, 10, 50)\n",
    "    #Hough_Lines = hough_lines(img, rho, theta, threshold, min_line_len=12, max_line_gap)\n",
    "\n",
    "    super_imposed=weighted_img(Hough_Lines, image, α=0.8, β=1, γ=0.)\n",
    "\n",
    "\n",
    "    return(super_imposed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init_Param(fifo_length=10):\n",
    "    \n",
    "        # parameters for the field of interest\n",
    "    global right_average_SLOPE,left_average_SLOPE, right_average_INTERCEPT,left_average_INTERCEPT\n",
    "    \n",
    "    right_average_SLOPE=AveragedBuffer(fifo_length)\n",
    "    left_average_SLOPE=AveragedBuffer(fifo_length)\n",
    "    right_average_INTERCEPT=AveragedBuffer(fifo_length)\n",
    "    left_average_INTERCEPT=AveragedBuffer(fifo_length)\n",
    "    \n",
    "        # parameters for polygon of interest \n",
    "    global YTOP,YBOT,BL,TL,VERTICES\n",
    "    \n",
    "    YTOP=340 #(mask top line : y ), will also be the top end of the extrapolated line \n",
    "    YBOT=540 #(mask bottom line : y),will also be the bottom end of the extrapolated line\n",
    "    BL=(40,YBOT) # bottom left 150\n",
    "    TL=(380,YTOP) # top left 430\n",
    "    TR=(580,YTOP) # top right 510\n",
    "    BR=(920,YBOT) # bottom right 900\n",
    "    VERTICES= np.array([[BL,TL,TR,BR]], dtype=np.int32)\n",
    "    \n",
    "        # parameteres for color filters\n",
    "    global lower_white,higher_white,lower_yellow,higher_yellow\n",
    "    \n",
    "    lower_yellow= HSL_color_picker(40,50,30)\n",
    "    higher_yellow=HSL_color_picker(70,100,83) \n",
    "    \n",
    "    lower_white=HSL_color_picker(0,0,75)\n",
    "    higher_white=HSL_color_picker(360,45,100)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidYellowLeft.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidYellowLeft.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 125/126 [00:10<00:00,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidYellowLeft.mp4 \n",
      "\n",
      "CPU times: user 5.14 s, sys: 154 ms, total: 5.29 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "#&&&&&&&&&&&&&&\n",
    "yellow_output = 'test_videos_output/solidYellowLeft.mp4'\n",
    "#white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "#clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\")\n",
    "clip2 = VideoFileClip(\"test_videos/solidYellowLeft.mp4\").subclip(0,5)\n",
    "Init_Param(10)\n",
    "yellow_clip = clip2.fl_image(process_image)#.subclip(0,5) #NOTE: this function expects color images!!\n",
    "#yellow_clip = clip2.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)\n",
    "#white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "#%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the video inline, or if you prefer find the video in your filesystem (should be in the same directory) and play it in your video player of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidYellowLeft.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))\n",
    "#\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the draw_lines() function\n",
    "\n",
    "**At this point, if you were successful with making the pipeline and tuning parameters, you probably have the Hough line segments drawn onto the road, but what about identifying the full extent of the lane and marking it clearly as in the example video (P1_example.mp4)?  Think about defining a line to run the full length of the visible lane based on the line segments you identified with the Hough Transform. As mentioned previously, try to average and/or extrapolate the line segments you've detected to map out the full extent of the lane lines. You can see an example of the result you're going for in the video \"P1_example.mp4\".**\n",
    "\n",
    "**Go back and modify your draw_lines function accordingly and try re-running your pipeline. The new output should draw a single, solid line over the left lane line and a single, solid line over the right lane line. The lines should start from the bottom of the image and extend out to the top of the region of interest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the one with the solid yellow lane on the left. This one's more tricky!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/solidWhiteRight.mp4\n",
      "[MoviePy] Writing video test_videos_output/solidWhiteRight.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 221/222 [00:09<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/solidWhiteRight.mp4 \n",
      "\n",
      "CPU times: user 8.28 s, sys: 900 ms, total: 9.18 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "white_output = 'test_videos_output/solidWhiteRight.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip2 = VideoFileClip('test_videos/solidYellowLeft.mp4').subclip(0,5)\n",
    "clip2 = VideoFileClip('test_videos/solidWhiteRight.mp4')\n",
    "Init_Param(10)\n",
    "white_clip = clip2.fl_image(process_image)\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/solidWhiteRighAverageBufferErraticFrameManagement.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup and Submission\n",
    "\n",
    "If you're satisfied with your video outputs, it's time to make the report writeup in a pdf or markdown file. Once you have this Ipython notebook ready along with the writeup, it's time to submit for review! Here is a [link](https://github.com/udacity/CarND-LaneLines-P1/blob/master/writeup_template.md) to the writeup template file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optional Challenge\n",
    "\n",
    "Try your lane finding pipeline on the video below.  Does it still work?  Can you figure out a way to make it more robust?  If you're up for the challenge, modify your pipeline so it works with this video and submit it along with the rest of your project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video test_videos_output/challenge.mp4\n",
      "[MoviePy] Writing video test_videos_output/challenge.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:14<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: test_videos_output/challenge.mp4 \n",
      "\n",
      "CPU times: user 11.7 s, sys: 1.31 s, total: 13 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "challenge_output = 'test_videos_output/challenge.mp4'\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip3 = VideoFileClip('test_videos/challenge.mp4').subclip(0,5)\n",
    "clip3 = VideoFileClip('test_videos/challenge.mp4')\n",
    "Init_Param(100)\n",
    "challenge_clip = clip3.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"test_videos_output/challenge.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
